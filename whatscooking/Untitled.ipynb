{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "oldsysstdout = sys.stdout\n",
    "class flushfile():\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "    def __getattr__(self,name): \n",
    "        return object.__getattribute__(self.f, name)\n",
    "    def write(self, x):\n",
    "        self.f.write(x)\n",
    "        self.f.flush()\n",
    "    def flush(self):\n",
    "        self.f.flush()\n",
    "sys.stdout = flushfile(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../12_fold_8_clf_train_probs.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../train.json', 'r') as f:\n",
    "    train = json.load(f)\n",
    "with open('../test.json', 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = map(lambda x: x['cuisine'], train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b, y = np.unique(target, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NB_0          8.691413e-14\n",
       "NB_1          1.004268e-18\n",
       "NB_2          5.267302e-11\n",
       "NB_3          2.099679e-20\n",
       "NB_4          1.092444e-14\n",
       "NB_5          8.658133e-12\n",
       "NB_6          9.998935e-01\n",
       "NB_7          9.498148e-16\n",
       "NB_8          1.224125e-18\n",
       "NB_9          1.660152e-06\n",
       "NB_10         3.296929e-16\n",
       "NB_11         3.329920e-21\n",
       "NB_12         3.078154e-20\n",
       "NB_13         1.048374e-04\n",
       "NB_14         1.021859e-11\n",
       "NB_15         1.602825e-16\n",
       "NB_16         8.762669e-14\n",
       "NB_17         2.181474e-09\n",
       "NB_18         1.161401e-18\n",
       "NB_19         3.242252e-18\n",
       "PAC_bag_0     0.000000e+00\n",
       "PAC_bag_1     0.000000e+00\n",
       "PAC_bag_2     0.000000e+00\n",
       "PAC_bag_3     0.000000e+00\n",
       "PAC_bag_4     0.000000e+00\n",
       "PAC_bag_5     0.000000e+00\n",
       "PAC_bag_6     1.000000e+00\n",
       "PAC_bag_7     0.000000e+00\n",
       "PAC_bag_8     0.000000e+00\n",
       "PAC_bag_9     0.000000e+00\n",
       "                  ...     \n",
       "SVC_bag_10    0.000000e+00\n",
       "SVC_bag_11    0.000000e+00\n",
       "SVC_bag_12    0.000000e+00\n",
       "SVC_bag_13    0.000000e+00\n",
       "SVC_bag_14    0.000000e+00\n",
       "SVC_bag_15    0.000000e+00\n",
       "SVC_bag_16    0.000000e+00\n",
       "SVC_bag_17    0.000000e+00\n",
       "SVC_bag_18    0.000000e+00\n",
       "SVC_bag_19    0.000000e+00\n",
       "XGB_0         1.059428e-04\n",
       "XGB_1         6.926775e-07\n",
       "XGB_2         9.182518e-05\n",
       "XGB_3         4.282053e-07\n",
       "XGB_4         4.592820e-05\n",
       "XGB_5         2.415801e-04\n",
       "XGB_6         9.984617e-01\n",
       "XGB_7         7.817826e-06\n",
       "XGB_8         2.891413e-06\n",
       "XGB_9         5.126560e-04\n",
       "XGB_10        1.067825e-06\n",
       "XGB_11        9.282384e-08\n",
       "XGB_12        4.201457e-07\n",
       "XGB_13        4.318586e-04\n",
       "XGB_14        4.134307e-06\n",
       "XGB_15        1.223141e-06\n",
       "XGB_16        7.919276e-05\n",
       "XGB_17        8.418035e-06\n",
       "XGB_18        2.339523e-07\n",
       "XGB_19        1.909859e-06\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.values.astype('float32')\n",
    "y = y.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 670 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DenseLayer, DropoutLayer\n",
    "from lasagne.utils import floatX\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(y, n_folds=12, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor3('inputs')\n",
    "target_var = T.ivector('targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 160)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt].reshape((-1, 1, 160)), targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... fold 1 of 10\n",
      "Epoch 1 of 50 took 0.172s\n",
      "  training loss:\t\t2.196316\n",
      "Epoch 2 of 50 took 0.165s\n",
      "  training loss:\t\t1.334671\n",
      "Epoch 3 of 50 took 0.152s\n",
      "  training loss:\t\t1.065298\n",
      "Epoch 4 of 50 took 0.148s\n",
      "  training loss:\t\t0.947153\n",
      "Epoch 5 of 50 took 0.146s\n",
      "  training loss:\t\t0.883328\n",
      "Epoch 6 of 50 took 0.147s\n",
      "  training loss:\t\t0.844075\n",
      "Epoch 7 of 50 took 0.149s\n",
      "  training loss:\t\t0.821449\n",
      "Epoch 8 of 50 took 0.166s\n",
      "  training loss:\t\t0.804069\n",
      "Epoch 9 of 50 took 0.166s\n",
      "  training loss:\t\t0.793019\n",
      "Epoch 10 of 50 took 0.153s\n",
      "  training loss:\t\t0.788725\n",
      "Epoch 11 of 50 took 0.164s\n",
      "  training loss:\t\t0.777361\n",
      "Epoch 12 of 50 took 0.154s\n",
      "  training loss:\t\t0.772614\n",
      "Epoch 13 of 50 took 0.155s\n",
      "  training loss:\t\t0.765787\n",
      "Epoch 14 of 50 took 0.164s\n",
      "  training loss:\t\t0.762143\n",
      "Epoch 15 of 50 took 0.152s\n",
      "  training loss:\t\t0.755816\n",
      "Epoch 16 of 50 took 0.159s\n",
      "  training loss:\t\t0.748885\n",
      "Epoch 17 of 50 took 0.152s\n",
      "  training loss:\t\t0.748893\n",
      "Epoch 18 of 50 took 0.151s\n",
      "  training loss:\t\t0.747677\n",
      "Epoch 19 of 50 took 0.160s\n",
      "  training loss:\t\t0.742744\n",
      "Epoch 20 of 50 took 0.152s\n",
      "  training loss:\t\t0.739725\n",
      "Epoch 21 of 50 took 0.147s\n",
      "  training loss:\t\t0.739605\n",
      "Epoch 22 of 50 took 0.157s\n",
      "  training loss:\t\t0.735137\n",
      "Epoch 23 of 50 took 0.150s\n",
      "  training loss:\t\t0.734121\n",
      "Epoch 24 of 50 took 0.147s\n",
      "  training loss:\t\t0.729249\n",
      "Epoch 25 of 50 took 0.154s\n",
      "  training loss:\t\t0.724591\n",
      "Epoch 26 of 50 took 0.157s\n",
      "  training loss:\t\t0.728350\n",
      "Epoch 27 of 50 took 0.154s\n",
      "  training loss:\t\t0.723795\n",
      "Epoch 28 of 50 took 0.147s\n",
      "  training loss:\t\t0.719562\n",
      "Epoch 29 of 50 took 0.149s\n",
      "  training loss:\t\t0.717526\n",
      "Epoch 30 of 50 took 0.159s\n",
      "  training loss:\t\t0.721164\n",
      "Epoch 31 of 50 took 0.151s\n",
      "  training loss:\t\t0.717573\n",
      "Epoch 32 of 50 took 0.149s\n",
      "  training loss:\t\t0.715392\n",
      "Epoch 33 of 50 took 0.160s\n",
      "  training loss:\t\t0.711402\n",
      "Epoch 34 of 50 took 0.153s\n",
      "  training loss:\t\t0.710454\n",
      "Epoch 35 of 50 took 0.150s\n",
      "  training loss:\t\t0.709266\n",
      "Epoch 36 of 50 took 0.160s\n",
      "  training loss:\t\t0.705970\n",
      "Epoch 37 of 50 took 0.152s\n",
      "  training loss:\t\t0.706445\n",
      "Epoch 38 of 50 took 0.151s\n",
      "  training loss:\t\t0.707355\n",
      "Epoch 39 of 50 took 0.160s\n",
      "  training loss:\t\t0.704032\n",
      "Epoch 40 of 50 took 0.152s\n",
      "  training loss:\t\t0.702476\n",
      "Epoch 41 of 50 took 0.148s\n",
      "  training loss:\t\t0.700420\n",
      "Epoch 42 of 50 took 0.159s\n",
      "  training loss:\t\t0.701170\n",
      "Epoch 43 of 50 took 0.154s\n",
      "  training loss:\t\t0.699025\n",
      "Epoch 44 of 50 took 0.147s\n",
      "  training loss:\t\t0.700907\n",
      "Epoch 45 of 50 took 0.150s\n",
      "  training loss:\t\t0.699782\n",
      "Epoch 46 of 50 took 0.158s\n",
      "  training loss:\t\t0.694102\n",
      "Epoch 47 of 50 took 0.155s\n",
      "  training loss:\t\t0.696528\n",
      "Epoch 48 of 50 took 0.148s\n",
      "  training loss:\t\t0.692195\n",
      "Epoch 49 of 50 took 0.147s\n",
      "  training loss:\t\t0.692440\n",
      "Epoch 50 of 50 took 0.158s\n",
      "  training loss:\t\t0.694776\n",
      "0.817880794702\n",
      "Starting training... fold 2 of 10\n",
      "Epoch 1 of 50 took 0.148s\n",
      "  training loss:\t\t2.165548\n",
      "Epoch 2 of 50 took 0.147s\n",
      "  training loss:\t\t1.302015\n",
      "Epoch 3 of 50 took 0.146s\n",
      "  training loss:\t\t1.057607\n",
      "Epoch 4 of 50 took 0.146s\n",
      "  training loss:\t\t0.938091\n",
      "Epoch 5 of 50 took 0.148s\n",
      "  training loss:\t\t0.883388\n",
      "Epoch 6 of 50 took 0.146s\n",
      "  training loss:\t\t0.842133\n",
      "Epoch 7 of 50 took 0.146s\n",
      "  training loss:\t\t0.819654\n",
      "Epoch 8 of 50 took 0.146s\n",
      "  training loss:\t\t0.800017\n",
      "Epoch 9 of 50 took 0.145s\n",
      "  training loss:\t\t0.791227\n",
      "Epoch 10 of 50 took 0.149s\n",
      "  training loss:\t\t0.780883\n",
      "Epoch 11 of 50 took 0.144s\n",
      "  training loss:\t\t0.773149\n",
      "Epoch 12 of 50 took 0.143s\n",
      "  training loss:\t\t0.773158\n",
      "Epoch 13 of 50 took 0.146s\n",
      "  training loss:\t\t0.766681\n",
      "Epoch 14 of 50 took 0.145s\n",
      "  training loss:\t\t0.759052\n",
      "Epoch 15 of 50 took 0.147s\n",
      "  training loss:\t\t0.753209\n",
      "Epoch 16 of 50 took 0.146s\n",
      "  training loss:\t\t0.751011\n",
      "Epoch 17 of 50 took 0.146s\n",
      "  training loss:\t\t0.746013\n",
      "Epoch 18 of 50 took 0.146s\n",
      "  training loss:\t\t0.746208\n",
      "Epoch 19 of 50 took 0.144s\n",
      "  training loss:\t\t0.741522\n",
      "Epoch 20 of 50 took 0.143s\n",
      "  training loss:\t\t0.733493\n",
      "Epoch 21 of 50 took 0.143s\n",
      "  training loss:\t\t0.732521\n",
      "Epoch 22 of 50 took 0.144s\n",
      "  training loss:\t\t0.731705\n",
      "Epoch 23 of 50 took 0.144s\n",
      "  training loss:\t\t0.727989\n",
      "Epoch 24 of 50 took 0.144s\n",
      "  training loss:\t\t0.727398\n",
      "Epoch 25 of 50 took 0.146s\n",
      "  training loss:\t\t0.722037\n",
      "Epoch 26 of 50 took 0.145s\n",
      "  training loss:\t\t0.725042\n",
      "Epoch 27 of 50 took 0.143s\n",
      "  training loss:\t\t0.719264\n",
      "Epoch 28 of 50 took 0.143s\n",
      "  training loss:\t\t0.715786\n",
      "Epoch 29 of 50 took 0.143s\n",
      "  training loss:\t\t0.715029\n",
      "Epoch 30 of 50 took 0.143s\n",
      "  training loss:\t\t0.712849\n",
      "Epoch 31 of 50 took 0.143s\n",
      "  training loss:\t\t0.712417\n",
      "Epoch 32 of 50 took 0.144s\n",
      "  training loss:\t\t0.712204\n",
      "Epoch 33 of 50 took 0.142s\n",
      "  training loss:\t\t0.714893\n",
      "Epoch 34 of 50 took 0.143s\n",
      "  training loss:\t\t0.707447\n",
      "Epoch 35 of 50 took 0.143s\n",
      "  training loss:\t\t0.709084\n",
      "Epoch 36 of 50 took 0.143s\n",
      "  training loss:\t\t0.706904\n",
      "Epoch 37 of 50 took 0.143s\n",
      "  training loss:\t\t0.710979\n",
      "Epoch 38 of 50 took 0.143s\n",
      "  training loss:\t\t0.703972\n",
      "Epoch 39 of 50 took 0.143s\n",
      "  training loss:\t\t0.704377\n",
      "Epoch 40 of 50 took 0.150s\n",
      "  training loss:\t\t0.703144\n",
      "Epoch 41 of 50 took 0.144s\n",
      "  training loss:\t\t0.699761\n",
      "Epoch 42 of 50 took 0.144s\n",
      "  training loss:\t\t0.700610\n",
      "Epoch 43 of 50 took 0.147s\n",
      "  training loss:\t\t0.700343\n",
      "Epoch 44 of 50 took 0.143s\n",
      "  training loss:\t\t0.696047\n",
      "Epoch 45 of 50 took 0.143s\n",
      "  training loss:\t\t0.697620\n",
      "Epoch 46 of 50 took 0.144s\n",
      "  training loss:\t\t0.698463\n",
      "Epoch 47 of 50 took 0.143s\n",
      "  training loss:\t\t0.698925\n",
      "Epoch 48 of 50 took 0.143s\n",
      "  training loss:\t\t0.695350\n",
      "Epoch 49 of 50 took 0.143s\n",
      "  training loss:\t\t0.695583\n",
      "Epoch 50 of 50 took 0.143s\n",
      "  training loss:\t\t0.689038\n",
      "0.812462372065\n",
      "Starting training... fold 3 of 10\n",
      "Epoch 1 of 50 took 0.145s\n",
      "  training loss:\t\t2.128163\n",
      "Epoch 2 of 50 took 0.145s\n",
      "  training loss:\t\t1.281547\n",
      "Epoch 3 of 50 took 0.147s\n",
      "  training loss:\t\t1.040077\n",
      "Epoch 4 of 50 took 0.145s\n",
      "  training loss:\t\t0.929675\n",
      "Epoch 5 of 50 took 0.144s\n",
      "  training loss:\t\t0.868452\n",
      "Epoch 6 of 50 took 0.144s\n",
      "  training loss:\t\t0.834300\n",
      "Epoch 7 of 50 took 0.144s\n",
      "  training loss:\t\t0.815553\n",
      "Epoch 8 of 50 took 0.144s\n",
      "  training loss:\t\t0.796578\n",
      "Epoch 9 of 50 took 0.144s\n",
      "  training loss:\t\t0.786994\n",
      "Epoch 10 of 50 took 0.144s\n",
      "  training loss:\t\t0.781652\n",
      "Epoch 11 of 50 took 0.144s\n",
      "  training loss:\t\t0.770039\n",
      "Epoch 12 of 50 took 0.145s\n",
      "  training loss:\t\t0.765388\n",
      "Epoch 13 of 50 took 0.143s\n",
      "  training loss:\t\t0.765250\n",
      "Epoch 14 of 50 took 0.143s\n",
      "  training loss:\t\t0.753503\n",
      "Epoch 15 of 50 took 0.143s\n",
      "  training loss:\t\t0.748284\n",
      "Epoch 16 of 50 took 0.143s\n",
      "  training loss:\t\t0.746404\n",
      "Epoch 17 of 50 took 0.143s\n",
      "  training loss:\t\t0.745559\n",
      "Epoch 18 of 50 took 0.144s\n",
      "  training loss:\t\t0.736091\n",
      "Epoch 19 of 50 took 0.143s\n",
      "  training loss:\t\t0.735947\n",
      "Epoch 20 of 50 took 0.143s\n",
      "  training loss:\t\t0.733129\n",
      "Epoch 21 of 50 took 0.144s\n",
      "  training loss:\t\t0.732716\n",
      "Epoch 22 of 50 took 0.143s\n",
      "  training loss:\t\t0.726941\n",
      "Epoch 23 of 50 took 0.143s\n",
      "  training loss:\t\t0.729777\n",
      "Epoch 24 of 50 took 0.144s\n",
      "  training loss:\t\t0.720831\n",
      "Epoch 25 of 50 took 0.143s\n",
      "  training loss:\t\t0.720850\n",
      "Epoch 26 of 50 took 0.143s\n",
      "  training loss:\t\t0.725265\n",
      "Epoch 27 of 50 took 0.143s\n",
      "  training loss:\t\t0.717647\n",
      "Epoch 28 of 50 took 0.143s\n",
      "  training loss:\t\t0.719030\n",
      "Epoch 29 of 50 took 0.143s\n",
      "  training loss:\t\t0.715183\n",
      "Epoch 30 of 50 took 0.143s\n",
      "  training loss:\t\t0.710173\n",
      "Epoch 31 of 50 took 0.143s\n",
      "  training loss:\t\t0.712545\n",
      "Epoch 32 of 50 took 0.143s\n",
      "  training loss:\t\t0.706926\n",
      "Epoch 33 of 50 took 0.143s\n",
      "  training loss:\t\t0.708432\n",
      "Epoch 34 of 50 took 0.143s\n",
      "  training loss:\t\t0.707548\n",
      "Epoch 35 of 50 took 0.144s\n",
      "  training loss:\t\t0.704580\n",
      "Epoch 36 of 50 took 0.143s\n",
      "  training loss:\t\t0.706315\n",
      "Epoch 37 of 50 took 0.143s\n",
      "  training loss:\t\t0.705889\n",
      "Epoch 38 of 50 took 0.142s\n",
      "  training loss:\t\t0.701533\n",
      "Epoch 39 of 50 took 0.143s\n",
      "  training loss:\t\t0.699229\n",
      "Epoch 40 of 50 took 0.144s\n",
      "  training loss:\t\t0.700483\n",
      "Epoch 41 of 50 took 0.145s\n",
      "  training loss:\t\t0.703139\n",
      "Epoch 42 of 50 took 0.145s\n",
      "  training loss:\t\t0.699014\n",
      "Epoch 43 of 50 took 0.144s\n",
      "  training loss:\t\t0.700119\n",
      "Epoch 44 of 50 took 0.144s\n",
      "  training loss:\t\t0.700617\n",
      "Epoch 45 of 50 took 0.143s\n",
      "  training loss:\t\t0.697446\n",
      "Epoch 46 of 50 took 0.143s\n",
      "  training loss:\t\t0.693449\n",
      "Epoch 47 of 50 took 0.143s\n",
      "  training loss:\t\t0.696208\n",
      "Epoch 48 of 50 took 0.143s\n",
      "  training loss:\t\t0.691096\n",
      "Epoch 49 of 50 took 0.143s\n",
      "  training loss:\t\t0.694294\n",
      "Epoch 50 of 50 took 0.143s\n",
      "  training loss:\t\t0.686208\n",
      "0.809337349398\n",
      "Starting training... fold 4 of 10\n",
      "Epoch 1 of 50 took 0.146s\n",
      "  training loss:\t\t2.180978\n",
      "Epoch 2 of 50 took 0.146s\n",
      "  training loss:\t\t1.275642\n",
      "Epoch 3 of 50 took 0.146s\n",
      "  training loss:\t\t1.040353\n",
      "Epoch 4 of 50 took 0.146s\n",
      "  training loss:\t\t0.935565\n",
      "Epoch 5 of 50 took 0.146s\n",
      "  training loss:\t\t0.872497\n",
      "Epoch 6 of 50 took 0.146s\n",
      "  training loss:\t\t0.839764\n",
      "Epoch 7 of 50 took 0.146s\n",
      "  training loss:\t\t0.824246\n",
      "Epoch 8 of 50 took 0.145s\n",
      "  training loss:\t\t0.800399\n",
      "Epoch 9 of 50 took 0.144s\n",
      "  training loss:\t\t0.788330\n",
      "Epoch 10 of 50 took 0.145s\n",
      "  training loss:\t\t0.780647\n",
      "Epoch 11 of 50 took 0.146s\n",
      "  training loss:\t\t0.774721\n",
      "Epoch 12 of 50 took 0.145s\n",
      "  training loss:\t\t0.768190\n",
      "Epoch 13 of 50 took 0.144s\n",
      "  training loss:\t\t0.766065\n",
      "Epoch 14 of 50 took 0.145s\n",
      "  training loss:\t\t0.757662\n",
      "Epoch 15 of 50 took 0.146s\n",
      "  training loss:\t\t0.749822\n",
      "Epoch 16 of 50 took 0.145s\n",
      "  training loss:\t\t0.749871\n",
      "Epoch 17 of 50 took 0.145s\n",
      "  training loss:\t\t0.743169\n",
      "Epoch 18 of 50 took 0.145s\n",
      "  training loss:\t\t0.740936\n",
      "Epoch 19 of 50 took 0.145s\n",
      "  training loss:\t\t0.740058\n",
      "Epoch 20 of 50 took 0.145s\n",
      "  training loss:\t\t0.740363\n",
      "Epoch 21 of 50 took 0.145s\n",
      "  training loss:\t\t0.733477\n",
      "Epoch 22 of 50 took 0.145s\n",
      "  training loss:\t\t0.730869\n",
      "Epoch 23 of 50 took 0.147s\n",
      "  training loss:\t\t0.730498\n",
      "Epoch 24 of 50 took 0.145s\n",
      "  training loss:\t\t0.724421\n",
      "Epoch 25 of 50 took 0.146s\n",
      "  training loss:\t\t0.722149\n",
      "Epoch 26 of 50 took 0.145s\n",
      "  training loss:\t\t0.724103\n",
      "Epoch 27 of 50 took 0.145s\n",
      "  training loss:\t\t0.722238\n",
      "Epoch 28 of 50 took 0.145s\n",
      "  training loss:\t\t0.718795\n",
      "Epoch 29 of 50 took 0.144s\n",
      "  training loss:\t\t0.714151\n",
      "Epoch 30 of 50 took 0.145s\n",
      "  training loss:\t\t0.712136\n",
      "Epoch 31 of 50 took 0.144s\n",
      "  training loss:\t\t0.713982\n",
      "Epoch 32 of 50 took 0.145s\n",
      "  training loss:\t\t0.710257\n",
      "Epoch 33 of 50 took 0.145s\n",
      "  training loss:\t\t0.711372\n",
      "Epoch 34 of 50 took 0.145s\n",
      "  training loss:\t\t0.710845\n",
      "Epoch 35 of 50 took 0.145s\n",
      "  training loss:\t\t0.707416\n",
      "Epoch 36 of 50 took 0.145s\n",
      "  training loss:\t\t0.706463\n",
      "Epoch 37 of 50 took 0.145s\n",
      "  training loss:\t\t0.706126\n",
      "Epoch 38 of 50 took 0.145s\n",
      "  training loss:\t\t0.706593\n",
      "Epoch 39 of 50 took 0.145s\n",
      "  training loss:\t\t0.702679\n",
      "Epoch 40 of 50 took 0.145s\n",
      "  training loss:\t\t0.697841\n",
      "Epoch 41 of 50 took 0.145s\n",
      "  training loss:\t\t0.696320\n",
      "Epoch 42 of 50 took 0.145s\n",
      "  training loss:\t\t0.694655\n",
      "Epoch 43 of 50 took 0.145s\n",
      "  training loss:\t\t0.703183\n",
      "Epoch 44 of 50 took 0.145s\n",
      "  training loss:\t\t0.696855\n",
      "Epoch 45 of 50 took 0.147s\n",
      "  training loss:\t\t0.694241\n",
      "Epoch 46 of 50 took 0.145s\n",
      "  training loss:\t\t0.691166\n",
      "Epoch 47 of 50 took 0.145s\n",
      "  training loss:\t\t0.690656\n",
      "Epoch 48 of 50 took 0.145s\n",
      "  training loss:\t\t0.695513\n",
      "Epoch 49 of 50 took 0.145s\n",
      "  training loss:\t\t0.692272\n",
      "Epoch 50 of 50 took 0.145s\n",
      "  training loss:\t\t0.688730\n",
      "0.808619650392\n",
      "Starting training... fold 5 of 10\n",
      "Epoch 1 of 50 took 0.145s\n",
      "  training loss:\t\t2.112592\n",
      "Epoch 2 of 50 took 0.144s\n",
      "  training loss:\t\t1.273621\n",
      "Epoch 3 of 50 took 0.145s\n",
      "  training loss:\t\t1.056416\n",
      "Epoch 4 of 50 took 0.146s\n",
      "  training loss:\t\t0.944426\n",
      "Epoch 5 of 50 took 0.146s\n",
      "  training loss:\t\t0.878586\n",
      "Epoch 6 of 50 took 0.145s\n",
      "  training loss:\t\t0.844009\n",
      "Epoch 7 of 50 took 0.145s\n",
      "  training loss:\t\t0.824019\n",
      "Epoch 8 of 50 took 0.144s\n",
      "  training loss:\t\t0.806174\n",
      "Epoch 9 of 50 took 0.150s\n",
      "  training loss:\t\t0.796160\n",
      "Epoch 10 of 50 took 0.144s\n",
      "  training loss:\t\t0.783856\n",
      "Epoch 11 of 50 took 0.147s\n",
      "  training loss:\t\t0.778065\n",
      "Epoch 12 of 50 took 0.146s\n",
      "  training loss:\t\t0.774102\n",
      "Epoch 13 of 50 took 0.145s\n",
      "  training loss:\t\t0.759842\n",
      "Epoch 14 of 50 took 0.145s\n",
      "  training loss:\t\t0.758199\n",
      "Epoch 15 of 50 took 0.144s\n",
      "  training loss:\t\t0.759283\n",
      "Epoch 16 of 50 took 0.144s\n",
      "  training loss:\t\t0.751773\n",
      "Epoch 17 of 50 took 0.144s\n",
      "  training loss:\t\t0.745370\n",
      "Epoch 18 of 50 took 0.145s\n",
      "  training loss:\t\t0.743699\n",
      "Epoch 19 of 50 took 0.145s\n",
      "  training loss:\t\t0.742761\n",
      "Epoch 20 of 50 took 0.144s\n",
      "  training loss:\t\t0.737859\n",
      "Epoch 21 of 50 took 0.144s\n",
      "  training loss:\t\t0.733924\n",
      "Epoch 22 of 50 took 0.144s\n",
      "  training loss:\t\t0.730971\n",
      "Epoch 23 of 50 took 0.144s\n",
      "  training loss:\t\t0.730347\n",
      "Epoch 24 of 50 took 0.146s\n",
      "  training loss:\t\t0.729080\n",
      "Epoch 25 of 50 took 0.145s\n",
      "  training loss:\t\t0.728712\n",
      "Epoch 26 of 50 took 0.144s\n",
      "  training loss:\t\t0.729591\n",
      "Epoch 27 of 50 took 0.144s\n",
      "  training loss:\t\t0.727395\n",
      "Epoch 28 of 50 took 0.144s\n",
      "  training loss:\t\t0.719951\n",
      "Epoch 29 of 50 took 0.144s\n",
      "  training loss:\t\t0.718029\n",
      "Epoch 30 of 50 took 0.144s\n",
      "  training loss:\t\t0.716076\n",
      "Epoch 31 of 50 took 0.145s\n",
      "  training loss:\t\t0.715138\n",
      "Epoch 32 of 50 took 0.144s\n",
      "  training loss:\t\t0.715144\n",
      "Epoch 33 of 50 took 0.144s\n",
      "  training loss:\t\t0.713319\n",
      "Epoch 34 of 50 took 0.145s\n",
      "  training loss:\t\t0.713422\n",
      "Epoch 35 of 50 took 0.144s\n",
      "  training loss:\t\t0.709460\n",
      "Epoch 36 of 50 took 0.144s\n",
      "  training loss:\t\t0.707601\n",
      "Epoch 37 of 50 took 0.144s\n",
      "  training loss:\t\t0.708632\n",
      "Epoch 38 of 50 took 0.144s\n",
      "  training loss:\t\t0.707988\n",
      "Epoch 39 of 50 took 0.144s\n",
      "  training loss:\t\t0.703403\n",
      "Epoch 40 of 50 took 0.145s\n",
      "  training loss:\t\t0.703552\n",
      "Epoch 41 of 50 took 0.144s\n",
      "  training loss:\t\t0.701663\n",
      "Epoch 42 of 50 took 0.144s\n",
      "  training loss:\t\t0.697876\n",
      "Epoch 43 of 50 took 0.144s\n",
      "  training loss:\t\t0.702663\n",
      "Epoch 44 of 50 took 0.144s\n",
      "  training loss:\t\t0.699173\n",
      "Epoch 45 of 50 took 0.144s\n",
      "  training loss:\t\t0.694011\n",
      "Epoch 46 of 50 took 0.144s\n",
      "  training loss:\t\t0.695600\n",
      "Epoch 47 of 50 took 0.144s\n",
      "  training loss:\t\t0.694198\n",
      "Epoch 48 of 50 took 0.145s\n",
      "  training loss:\t\t0.696407\n",
      "Epoch 49 of 50 took 0.147s\n",
      "  training loss:\t\t0.695485\n",
      "Epoch 50 of 50 took 0.144s\n",
      "  training loss:\t\t0.693642\n",
      "0.813743218807\n",
      "Starting training... fold 6 of 10\n",
      "Epoch 1 of 50 took 0.146s\n",
      "  training loss:\t\t2.279238\n",
      "Epoch 2 of 50 took 0.146s\n",
      "  training loss:\t\t1.321547\n",
      "Epoch 3 of 50 took 0.146s\n",
      "  training loss:\t\t1.069661\n",
      "Epoch 4 of 50 took 0.146s\n",
      "  training loss:\t\t0.951951\n",
      "Epoch 5 of 50 took 0.146s\n",
      "  training loss:\t\t0.885557\n",
      "Epoch 6 of 50 took 0.146s\n",
      "  training loss:\t\t0.851335\n",
      "Epoch 7 of 50 took 0.146s\n",
      "  training loss:\t\t0.823826\n",
      "Epoch 8 of 50 took 0.146s\n",
      "  training loss:\t\t0.809614\n",
      "Epoch 9 of 50 took 0.146s\n",
      "  training loss:\t\t0.795386\n",
      "Epoch 10 of 50 took 0.146s\n",
      "  training loss:\t\t0.786424\n",
      "Epoch 11 of 50 took 0.146s\n",
      "  training loss:\t\t0.773359\n",
      "Epoch 12 of 50 took 0.147s\n",
      "  training loss:\t\t0.769183\n",
      "Epoch 13 of 50 took 0.146s\n",
      "  training loss:\t\t0.759662\n",
      "Epoch 14 of 50 took 0.146s\n",
      "  training loss:\t\t0.759752\n",
      "Epoch 15 of 50 took 0.146s\n",
      "  training loss:\t\t0.754124\n",
      "Epoch 16 of 50 took 0.145s\n",
      "  training loss:\t\t0.750841\n",
      "Epoch 17 of 50 took 0.146s\n",
      "  training loss:\t\t0.744958\n",
      "Epoch 18 of 50 took 0.146s\n",
      "  training loss:\t\t0.740000\n",
      "Epoch 19 of 50 took 0.146s\n",
      "  training loss:\t\t0.737693\n",
      "Epoch 20 of 50 took 0.146s\n",
      "  training loss:\t\t0.739566\n",
      "Epoch 21 of 50 took 0.146s\n",
      "  training loss:\t\t0.732779\n",
      "Epoch 22 of 50 took 0.146s\n",
      "  training loss:\t\t0.733087\n",
      "Epoch 23 of 50 took 0.146s\n",
      "  training loss:\t\t0.725205\n",
      "Epoch 24 of 50 took 0.146s\n",
      "  training loss:\t\t0.727465\n",
      "Epoch 25 of 50 took 0.146s\n",
      "  training loss:\t\t0.724766\n",
      "Epoch 26 of 50 took 0.146s\n",
      "  training loss:\t\t0.720287\n",
      "Epoch 27 of 50 took 0.146s\n",
      "  training loss:\t\t0.720575\n",
      "Epoch 28 of 50 took 0.146s\n",
      "  training loss:\t\t0.720263\n",
      "Epoch 29 of 50 took 0.148s\n",
      "  training loss:\t\t0.719171\n",
      "Epoch 30 of 50 took 0.148s\n",
      "  training loss:\t\t0.717747\n",
      "Epoch 31 of 50 took 0.146s\n",
      "  training loss:\t\t0.714740\n",
      "Epoch 32 of 50 took 0.148s\n",
      "  training loss:\t\t0.713177\n",
      "Epoch 33 of 50 took 0.146s\n",
      "  training loss:\t\t0.711087\n",
      "Epoch 34 of 50 took 0.146s\n",
      "  training loss:\t\t0.711675\n",
      "Epoch 35 of 50 took 0.146s\n",
      "  training loss:\t\t0.704069\n",
      "Epoch 36 of 50 took 0.146s\n",
      "  training loss:\t\t0.707066\n",
      "Epoch 37 of 50 took 0.147s\n",
      "  training loss:\t\t0.701799\n",
      "Epoch 38 of 50 took 0.146s\n",
      "  training loss:\t\t0.702816\n",
      "Epoch 39 of 50 took 0.147s\n",
      "  training loss:\t\t0.702626\n",
      "Epoch 40 of 50 took 0.146s\n",
      "  training loss:\t\t0.704796\n",
      "Epoch 41 of 50 took 0.146s\n",
      "  training loss:\t\t0.705326\n",
      "Epoch 42 of 50 took 0.146s\n",
      "  training loss:\t\t0.696125\n",
      "Epoch 43 of 50 took 0.151s\n",
      "  training loss:\t\t0.698039\n",
      "Epoch 44 of 50 took 0.146s\n",
      "  training loss:\t\t0.699180\n",
      "Epoch 45 of 50 took 0.146s\n",
      "  training loss:\t\t0.695172\n",
      "Epoch 46 of 50 took 0.146s\n",
      "  training loss:\t\t0.696868\n",
      "Epoch 47 of 50 took 0.146s\n",
      "  training loss:\t\t0.696236\n",
      "Epoch 48 of 50 took 0.146s\n",
      "  training loss:\t\t0.691323\n",
      "Epoch 49 of 50 took 0.146s\n",
      "  training loss:\t\t0.689808\n",
      "Epoch 50 of 50 took 0.146s\n",
      "  training loss:\t\t0.694199\n",
      "0.807297949337\n",
      "Starting training... fold 7 of 10\n",
      "Epoch 1 of 50 took 0.145s\n",
      "  training loss:\t\t2.144582\n",
      "Epoch 2 of 50 took 0.145s\n",
      "  training loss:\t\t1.292320\n",
      "Epoch 3 of 50 took 0.145s\n",
      "  training loss:\t\t1.056660\n",
      "Epoch 4 of 50 took 0.144s\n",
      "  training loss:\t\t0.941565\n",
      "Epoch 5 of 50 took 0.145s\n",
      "  training loss:\t\t0.883992\n",
      "Epoch 6 of 50 took 0.145s\n",
      "  training loss:\t\t0.842148\n",
      "Epoch 7 of 50 took 0.147s\n",
      "  training loss:\t\t0.818382\n",
      "Epoch 8 of 50 took 0.145s\n",
      "  training loss:\t\t0.800929\n",
      "Epoch 9 of 50 took 0.145s\n",
      "  training loss:\t\t0.792031\n",
      "Epoch 10 of 50 took 0.144s\n",
      "  training loss:\t\t0.785575\n",
      "Epoch 11 of 50 took 0.145s\n",
      "  training loss:\t\t0.773731\n",
      "Epoch 12 of 50 took 0.146s\n",
      "  training loss:\t\t0.769610\n",
      "Epoch 13 of 50 took 0.145s\n",
      "  training loss:\t\t0.761344\n",
      "Epoch 14 of 50 took 0.149s\n",
      "  training loss:\t\t0.761901\n",
      "Epoch 15 of 50 took 0.150s\n",
      "  training loss:\t\t0.755635\n",
      "Epoch 16 of 50 took 0.148s\n",
      "  training loss:\t\t0.750810\n",
      "Epoch 17 of 50 took 0.151s\n",
      "  training loss:\t\t0.744752\n",
      "Epoch 18 of 50 took 0.149s\n",
      "  training loss:\t\t0.745024\n",
      "Epoch 19 of 50 took 0.149s\n",
      "  training loss:\t\t0.738035\n",
      "Epoch 20 of 50 took 0.147s\n",
      "  training loss:\t\t0.737352\n",
      "Epoch 21 of 50 took 0.149s\n",
      "  training loss:\t\t0.734110\n",
      "Epoch 22 of 50 took 0.148s\n",
      "  training loss:\t\t0.730937\n",
      "Epoch 23 of 50 took 0.148s\n",
      "  training loss:\t\t0.729545\n",
      "Epoch 24 of 50 took 0.148s\n",
      "  training loss:\t\t0.721323\n",
      "Epoch 25 of 50 took 0.149s\n",
      "  training loss:\t\t0.722466\n",
      "Epoch 26 of 50 took 0.148s\n",
      "  training loss:\t\t0.722257\n",
      "Epoch 27 of 50 took 0.149s\n",
      "  training loss:\t\t0.717357\n",
      "Epoch 28 of 50 took 0.147s\n",
      "  training loss:\t\t0.722152\n",
      "Epoch 29 of 50 took 0.148s\n",
      "  training loss:\t\t0.720707\n",
      "Epoch 30 of 50 took 0.149s\n",
      "  training loss:\t\t0.718528\n",
      "Epoch 31 of 50 took 0.145s\n",
      "  training loss:\t\t0.712271\n",
      "Epoch 32 of 50 took 0.144s\n",
      "  training loss:\t\t0.713247\n",
      "Epoch 33 of 50 took 0.145s\n",
      "  training loss:\t\t0.710065\n",
      "Epoch 34 of 50 took 0.145s\n",
      "  training loss:\t\t0.711898\n",
      "Epoch 35 of 50 took 0.145s\n",
      "  training loss:\t\t0.707158\n",
      "Epoch 36 of 50 took 0.144s\n",
      "  training loss:\t\t0.705997\n",
      "Epoch 37 of 50 took 0.145s\n",
      "  training loss:\t\t0.706499\n",
      "Epoch 38 of 50 took 0.144s\n",
      "  training loss:\t\t0.705107\n",
      "Epoch 39 of 50 took 0.145s\n",
      "  training loss:\t\t0.701327\n",
      "Epoch 40 of 50 took 0.147s\n",
      "  training loss:\t\t0.701895\n",
      "Epoch 41 of 50 took 0.148s\n",
      "  training loss:\t\t0.699368\n",
      "Epoch 42 of 50 took 0.149s\n",
      "  training loss:\t\t0.698897\n",
      "Epoch 43 of 50 took 0.148s\n",
      "  training loss:\t\t0.701314\n",
      "Epoch 44 of 50 took 0.147s\n",
      "  training loss:\t\t0.700108\n",
      "Epoch 45 of 50 took 0.146s\n",
      "  training loss:\t\t0.698364\n",
      "Epoch 46 of 50 took 0.147s\n",
      "  training loss:\t\t0.700091\n",
      "Epoch 47 of 50 took 0.147s\n",
      "  training loss:\t\t0.698587\n",
      "Epoch 48 of 50 took 0.145s\n",
      "  training loss:\t\t0.696401\n",
      "Epoch 49 of 50 took 0.145s\n",
      "  training loss:\t\t0.695587\n",
      "Epoch 50 of 50 took 0.145s\n",
      "  training loss:\t\t0.689413\n",
      "0.815027157514\n",
      "Starting training... fold 8 of 10\n",
      "Epoch 1 of 50 took 0.144s\n",
      "  training loss:\t\t2.119913\n",
      "Epoch 2 of 50 took 0.145s\n",
      "  training loss:\t\t1.281794\n",
      "Epoch 3 of 50 took 0.144s\n",
      "  training loss:\t\t1.047723\n",
      "Epoch 4 of 50 took 0.145s\n",
      "  training loss:\t\t0.935287\n",
      "Epoch 5 of 50 took 0.144s\n",
      "  training loss:\t\t0.871884\n",
      "Epoch 6 of 50 took 0.145s\n",
      "  training loss:\t\t0.840732\n",
      "Epoch 7 of 50 took 0.144s\n",
      "  training loss:\t\t0.813600\n",
      "Epoch 8 of 50 took 0.146s\n",
      "  training loss:\t\t0.798085\n",
      "Epoch 9 of 50 took 0.144s\n",
      "  training loss:\t\t0.792085\n",
      "Epoch 10 of 50 took 0.144s\n",
      "  training loss:\t\t0.775467\n",
      "Epoch 11 of 50 took 0.145s\n",
      "  training loss:\t\t0.768383\n",
      "Epoch 12 of 50 took 0.146s\n",
      "  training loss:\t\t0.765742\n",
      "Epoch 13 of 50 took 0.144s\n",
      "  training loss:\t\t0.760488\n",
      "Epoch 14 of 50 took 0.144s\n",
      "  training loss:\t\t0.750542\n",
      "Epoch 15 of 50 took 0.145s\n",
      "  training loss:\t\t0.752204\n",
      "Epoch 16 of 50 took 0.144s\n",
      "  training loss:\t\t0.746328\n",
      "Epoch 17 of 50 took 0.144s\n",
      "  training loss:\t\t0.748854\n",
      "Epoch 18 of 50 took 0.144s\n",
      "  training loss:\t\t0.738224\n",
      "Epoch 19 of 50 took 0.144s\n",
      "  training loss:\t\t0.742148\n",
      "Epoch 20 of 50 took 0.145s\n",
      "  training loss:\t\t0.736370\n",
      "Epoch 21 of 50 took 0.144s\n",
      "  training loss:\t\t0.732004\n",
      "Epoch 22 of 50 took 0.144s\n",
      "  training loss:\t\t0.729272\n",
      "Epoch 23 of 50 took 0.144s\n",
      "  training loss:\t\t0.726065\n",
      "Epoch 24 of 50 took 0.145s\n",
      "  training loss:\t\t0.731526\n",
      "Epoch 25 of 50 took 0.145s\n",
      "  training loss:\t\t0.728662\n",
      "Epoch 26 of 50 took 0.144s\n",
      "  training loss:\t\t0.719192\n",
      "Epoch 27 of 50 took 0.144s\n",
      "  training loss:\t\t0.721569\n",
      "Epoch 28 of 50 took 0.145s\n",
      "  training loss:\t\t0.718000\n",
      "Epoch 29 of 50 took 0.145s\n",
      "  training loss:\t\t0.715958\n",
      "Epoch 30 of 50 took 0.144s\n",
      "  training loss:\t\t0.713290\n",
      "Epoch 31 of 50 took 0.146s\n",
      "  training loss:\t\t0.715299\n",
      "Epoch 32 of 50 took 0.145s\n",
      "  training loss:\t\t0.712598\n",
      "Epoch 33 of 50 took 0.145s\n",
      "  training loss:\t\t0.708314\n",
      "Epoch 34 of 50 took 0.144s\n",
      "  training loss:\t\t0.708585\n",
      "Epoch 35 of 50 took 0.144s\n",
      "  training loss:\t\t0.704499\n",
      "Epoch 36 of 50 took 0.145s\n",
      "  training loss:\t\t0.703084\n",
      "Epoch 37 of 50 took 0.145s\n",
      "  training loss:\t\t0.709248\n",
      "Epoch 38 of 50 took 0.146s\n",
      "  training loss:\t\t0.703304\n",
      "Epoch 39 of 50 took 0.145s\n",
      "  training loss:\t\t0.699158\n",
      "Epoch 40 of 50 took 0.145s\n",
      "  training loss:\t\t0.700616\n",
      "Epoch 41 of 50 took 0.144s\n",
      "  training loss:\t\t0.698093\n",
      "Epoch 42 of 50 took 0.144s\n",
      "  training loss:\t\t0.696532\n",
      "Epoch 43 of 50 took 0.144s\n",
      "  training loss:\t\t0.693194\n",
      "Epoch 44 of 50 took 0.144s\n",
      "  training loss:\t\t0.694037\n",
      "Epoch 45 of 50 took 0.145s\n",
      "  training loss:\t\t0.695946\n",
      "Epoch 46 of 50 took 0.145s\n",
      "  training loss:\t\t0.692682\n",
      "Epoch 47 of 50 took 0.144s\n",
      "  training loss:\t\t0.690652\n",
      "Epoch 48 of 50 took 0.144s\n",
      "  training loss:\t\t0.690432\n",
      "Epoch 49 of 50 took 0.145s\n",
      "  training loss:\t\t0.689890\n",
      "Epoch 50 of 50 took 0.144s\n",
      "  training loss:\t\t0.691793\n",
      "0.813405797101\n",
      "Starting training... fold 9 of 10\n",
      "Epoch 1 of 50 took 0.146s\n",
      "  training loss:\t\t2.167389\n",
      "Epoch 2 of 50 took 0.146s\n",
      "  training loss:\t\t1.298619\n",
      "Epoch 3 of 50 took 0.146s\n",
      "  training loss:\t\t1.037573\n",
      "Epoch 4 of 50 took 0.146s\n",
      "  training loss:\t\t0.926814\n",
      "Epoch 5 of 50 took 0.146s\n",
      "  training loss:\t\t0.873528\n",
      "Epoch 6 of 50 took 0.146s\n",
      "  training loss:\t\t0.836506\n",
      "Epoch 7 of 50 took 0.157s\n",
      "  training loss:\t\t0.819049\n",
      "Epoch 8 of 50 took 0.153s\n",
      "  training loss:\t\t0.801378\n",
      "Epoch 9 of 50 took 0.146s\n",
      "  training loss:\t\t0.790649\n",
      "Epoch 10 of 50 took 0.147s\n",
      "  training loss:\t\t0.782970\n",
      "Epoch 11 of 50 took 0.146s\n",
      "  training loss:\t\t0.778056\n",
      "Epoch 12 of 50 took 0.146s\n",
      "  training loss:\t\t0.773304\n",
      "Epoch 13 of 50 took 0.148s\n",
      "  training loss:\t\t0.763560\n",
      "Epoch 14 of 50 took 0.146s\n",
      "  training loss:\t\t0.762646\n",
      "Epoch 15 of 50 took 0.146s\n",
      "  training loss:\t\t0.757020\n",
      "Epoch 16 of 50 took 0.146s\n",
      "  training loss:\t\t0.753731\n",
      "Epoch 17 of 50 took 0.146s\n",
      "  training loss:\t\t0.745761\n",
      "Epoch 18 of 50 took 0.146s\n",
      "  training loss:\t\t0.748442\n",
      "Epoch 19 of 50 took 0.147s\n",
      "  training loss:\t\t0.741752\n",
      "Epoch 20 of 50 took 0.148s\n",
      "  training loss:\t\t0.739516\n",
      "Epoch 21 of 50 took 0.148s\n",
      "  training loss:\t\t0.734931\n",
      "Epoch 22 of 50 took 0.148s\n",
      "  training loss:\t\t0.731965\n",
      "Epoch 23 of 50 took 0.147s\n",
      "  training loss:\t\t0.728155\n",
      "Epoch 24 of 50 took 0.148s\n",
      "  training loss:\t\t0.730673\n",
      "Epoch 25 of 50 took 0.146s\n",
      "  training loss:\t\t0.728015\n",
      "Epoch 26 of 50 took 0.147s\n",
      "  training loss:\t\t0.722238\n",
      "Epoch 27 of 50 took 0.146s\n",
      "  training loss:\t\t0.719065\n",
      "Epoch 28 of 50 took 0.146s\n",
      "  training loss:\t\t0.719471\n",
      "Epoch 29 of 50 took 0.148s\n",
      "  training loss:\t\t0.717453\n",
      "Epoch 30 of 50 took 0.147s\n",
      "  training loss:\t\t0.717826\n",
      "Epoch 31 of 50 took 0.146s\n",
      "  training loss:\t\t0.718707\n",
      "Epoch 32 of 50 took 0.146s\n",
      "  training loss:\t\t0.716552\n",
      "Epoch 33 of 50 took 0.147s\n",
      "  training loss:\t\t0.714638\n",
      "Epoch 34 of 50 took 0.146s\n",
      "  training loss:\t\t0.711944\n",
      "Epoch 35 of 50 took 0.147s\n",
      "  training loss:\t\t0.709492\n",
      "Epoch 36 of 50 took 0.147s\n",
      "  training loss:\t\t0.705986\n",
      "Epoch 37 of 50 took 0.146s\n",
      "  training loss:\t\t0.707056\n",
      "Epoch 38 of 50 took 0.146s\n",
      "  training loss:\t\t0.709343\n",
      "Epoch 39 of 50 took 0.146s\n",
      "  training loss:\t\t0.703117\n",
      "Epoch 40 of 50 took 0.147s\n",
      "  training loss:\t\t0.703320\n",
      "Epoch 41 of 50 took 0.146s\n",
      "  training loss:\t\t0.706007\n",
      "Epoch 42 of 50 took 0.146s\n",
      "  training loss:\t\t0.701483\n",
      "Epoch 43 of 50 took 0.146s\n",
      "  training loss:\t\t0.700845\n",
      "Epoch 44 of 50 took 0.146s\n",
      "  training loss:\t\t0.698426\n",
      "Epoch 45 of 50 took 0.146s\n",
      "  training loss:\t\t0.698938\n",
      "Epoch 46 of 50 took 0.148s\n",
      "  training loss:\t\t0.698587\n",
      "Epoch 47 of 50 took 0.148s\n",
      "  training loss:\t\t0.697812\n",
      "Epoch 48 of 50 took 0.147s\n",
      "  training loss:\t\t0.699253\n",
      "Epoch 49 of 50 took 0.147s\n",
      "  training loss:\t\t0.691495\n",
      "Epoch 50 of 50 took 0.146s\n",
      "  training loss:\t\t0.693432\n",
      "0.818538647343\n",
      "Starting training... fold 10 of 10\n",
      "Epoch 1 of 50 took 0.145s\n",
      "  training loss:\t\t2.139856\n",
      "Epoch 2 of 50 took 0.144s\n",
      "  training loss:\t\t1.287059\n",
      "Epoch 3 of 50 took 0.144s\n",
      "  training loss:\t\t1.048200\n",
      "Epoch 4 of 50 took 0.144s\n",
      "  training loss:\t\t0.935075\n",
      "Epoch 5 of 50 took 0.146s\n",
      "  training loss:\t\t0.876851\n",
      "Epoch 6 of 50 took 0.147s\n",
      "  training loss:\t\t0.843737\n",
      "Epoch 7 of 50 took 0.145s\n",
      "  training loss:\t\t0.820205\n",
      "Epoch 8 of 50 took 0.147s\n",
      "  training loss:\t\t0.807217\n",
      "Epoch 9 of 50 took 0.145s\n",
      "  training loss:\t\t0.794539\n",
      "Epoch 10 of 50 took 0.145s\n",
      "  training loss:\t\t0.783510\n",
      "Epoch 11 of 50 took 0.145s\n",
      "  training loss:\t\t0.776003\n",
      "Epoch 12 of 50 took 0.144s\n",
      "  training loss:\t\t0.772300\n",
      "Epoch 13 of 50 took 0.144s\n",
      "  training loss:\t\t0.765928\n",
      "Epoch 14 of 50 took 0.145s\n",
      "  training loss:\t\t0.757479\n",
      "Epoch 15 of 50 took 0.145s\n",
      "  training loss:\t\t0.760323\n",
      "Epoch 16 of 50 took 0.145s\n",
      "  training loss:\t\t0.749735\n",
      "Epoch 17 of 50 took 0.144s\n",
      "  training loss:\t\t0.746728\n",
      "Epoch 18 of 50 took 0.145s\n",
      "  training loss:\t\t0.744831\n",
      "Epoch 19 of 50 took 0.145s\n",
      "  training loss:\t\t0.740146\n",
      "Epoch 20 of 50 took 0.145s\n",
      "  training loss:\t\t0.738214\n",
      "Epoch 21 of 50 took 0.145s\n",
      "  training loss:\t\t0.736806\n",
      "Epoch 22 of 50 took 0.145s\n",
      "  training loss:\t\t0.733872\n",
      "Epoch 23 of 50 took 0.145s\n",
      "  training loss:\t\t0.735735\n",
      "Epoch 24 of 50 took 0.145s\n",
      "  training loss:\t\t0.726873\n",
      "Epoch 25 of 50 took 0.145s\n",
      "  training loss:\t\t0.725830\n",
      "Epoch 26 of 50 took 0.145s\n",
      "  training loss:\t\t0.725356\n",
      "Epoch 27 of 50 took 0.145s\n",
      "  training loss:\t\t0.723472\n",
      "Epoch 28 of 50 took 0.144s\n",
      "  training loss:\t\t0.717928\n",
      "Epoch 29 of 50 took 0.144s\n",
      "  training loss:\t\t0.720379\n",
      "Epoch 30 of 50 took 0.144s\n",
      "  training loss:\t\t0.713943\n",
      "Epoch 31 of 50 took 0.144s\n",
      "  training loss:\t\t0.714946\n",
      "Epoch 32 of 50 took 0.144s\n",
      "  training loss:\t\t0.713689\n",
      "Epoch 33 of 50 took 0.145s\n",
      "  training loss:\t\t0.709792\n",
      "Epoch 34 of 50 took 0.144s\n",
      "  training loss:\t\t0.711207\n",
      "Epoch 35 of 50 took 0.144s\n",
      "  training loss:\t\t0.706835\n",
      "Epoch 36 of 50 took 0.144s\n",
      "  training loss:\t\t0.705662\n",
      "Epoch 37 of 50 took 0.144s\n",
      "  training loss:\t\t0.709770\n",
      "Epoch 38 of 50 took 0.145s\n",
      "  training loss:\t\t0.705052\n",
      "Epoch 39 of 50 took 0.144s\n",
      "  training loss:\t\t0.702491\n",
      "Epoch 40 of 50 took 0.145s\n",
      "  training loss:\t\t0.703881\n",
      "Epoch 41 of 50 took 0.152s\n",
      "  training loss:\t\t0.701439\n",
      "Epoch 42 of 50 took 0.145s\n",
      "  training loss:\t\t0.703212\n",
      "Epoch 43 of 50 took 0.145s\n",
      "  training loss:\t\t0.700713\n",
      "Epoch 44 of 50 took 0.145s\n",
      "  training loss:\t\t0.696918\n",
      "Epoch 45 of 50 took 0.145s\n",
      "  training loss:\t\t0.699624\n",
      "Epoch 46 of 50 took 0.145s\n",
      "  training loss:\t\t0.696703\n",
      "Epoch 47 of 50 took 0.145s\n",
      "  training loss:\t\t0.700668\n",
      "Epoch 48 of 50 took 0.144s\n",
      "  training loss:\t\t0.695603\n",
      "Epoch 49 of 50 took 0.145s\n",
      "  training loss:\t\t0.688112\n",
      "Epoch 50 of 50 took 0.147s\n",
      "  training loss:\t\t0.688683\n",
      "0.812330009066\n",
      "Starting training... fold 11 of 10\n",
      "Epoch 1 of 50 took 0.147s\n",
      "  training loss:\t\t2.094387\n",
      "Epoch 2 of 50 took 0.146s\n",
      "  training loss:\t\t1.270327\n",
      "Epoch 3 of 50 took 0.147s\n",
      "  training loss:\t\t1.029060\n",
      "Epoch 4 of 50 took 0.148s\n",
      "  training loss:\t\t0.923900\n",
      "Epoch 5 of 50 took 0.153s\n",
      "  training loss:\t\t0.865735\n",
      "Epoch 6 of 50 took 0.152s\n",
      "  training loss:\t\t0.827950\n",
      "Epoch 7 of 50 took 0.149s\n",
      "  training loss:\t\t0.811286\n",
      "Epoch 8 of 50 took 0.146s\n",
      "  training loss:\t\t0.799855\n",
      "Epoch 9 of 50 took 0.146s\n",
      "  training loss:\t\t0.779448\n",
      "Epoch 10 of 50 took 0.146s\n",
      "  training loss:\t\t0.780983\n",
      "Epoch 11 of 50 took 0.147s\n",
      "  training loss:\t\t0.773852\n",
      "Epoch 12 of 50 took 0.147s\n",
      "  training loss:\t\t0.765858\n",
      "Epoch 13 of 50 took 0.147s\n",
      "  training loss:\t\t0.759721\n",
      "Epoch 14 of 50 took 0.146s\n",
      "  training loss:\t\t0.760678\n",
      "Epoch 15 of 50 took 0.146s\n",
      "  training loss:\t\t0.752492\n",
      "Epoch 16 of 50 took 0.147s\n",
      "  training loss:\t\t0.747430\n",
      "Epoch 17 of 50 took 0.147s\n",
      "  training loss:\t\t0.747042\n",
      "Epoch 18 of 50 took 0.148s\n",
      "  training loss:\t\t0.741620\n",
      "Epoch 19 of 50 took 0.147s\n",
      "  training loss:\t\t0.734881\n",
      "Epoch 20 of 50 took 0.147s\n",
      "  training loss:\t\t0.736595\n",
      "Epoch 21 of 50 took 0.148s\n",
      "  training loss:\t\t0.733583\n",
      "Epoch 22 of 50 took 0.148s\n",
      "  training loss:\t\t0.735120\n",
      "Epoch 23 of 50 took 0.147s\n",
      "  training loss:\t\t0.732422\n",
      "Epoch 24 of 50 took 0.147s\n",
      "  training loss:\t\t0.728469\n",
      "Epoch 25 of 50 took 0.147s\n",
      "  training loss:\t\t0.725326\n",
      "Epoch 26 of 50 took 0.147s\n",
      "  training loss:\t\t0.720170\n",
      "Epoch 27 of 50 took 0.149s\n",
      "  training loss:\t\t0.721572\n",
      "Epoch 28 of 50 took 0.149s\n",
      "  training loss:\t\t0.717413\n",
      "Epoch 29 of 50 took 0.147s\n",
      "  training loss:\t\t0.718156\n",
      "Epoch 30 of 50 took 0.147s\n",
      "  training loss:\t\t0.717826\n",
      "Epoch 31 of 50 took 0.147s\n",
      "  training loss:\t\t0.710647\n",
      "Epoch 32 of 50 took 0.147s\n",
      "  training loss:\t\t0.712166\n",
      "Epoch 33 of 50 took 0.148s\n",
      "  training loss:\t\t0.713626\n",
      "Epoch 34 of 50 took 0.147s\n",
      "  training loss:\t\t0.710092\n",
      "Epoch 35 of 50 took 0.147s\n",
      "  training loss:\t\t0.706015\n",
      "Epoch 36 of 50 took 0.147s\n",
      "  training loss:\t\t0.708872\n",
      "Epoch 37 of 50 took 0.146s\n",
      "  training loss:\t\t0.705897\n",
      "Epoch 38 of 50 took 0.147s\n",
      "  training loss:\t\t0.703311\n",
      "Epoch 39 of 50 took 0.146s\n",
      "  training loss:\t\t0.701652\n",
      "Epoch 40 of 50 took 0.146s\n",
      "  training loss:\t\t0.701783\n",
      "Epoch 41 of 50 took 0.146s\n",
      "  training loss:\t\t0.700745\n",
      "Epoch 42 of 50 took 0.146s\n",
      "  training loss:\t\t0.698331\n",
      "Epoch 43 of 50 took 0.147s\n",
      "  training loss:\t\t0.698903\n",
      "Epoch 44 of 50 took 0.146s\n",
      "  training loss:\t\t0.701790\n",
      "Epoch 45 of 50 took 0.151s\n",
      "  training loss:\t\t0.697356\n",
      "Epoch 46 of 50 took 0.148s\n",
      "  training loss:\t\t0.693355\n",
      "Epoch 47 of 50 took 0.147s\n",
      "  training loss:\t\t0.695417\n",
      "Epoch 48 of 50 took 0.147s\n",
      "  training loss:\t\t0.692554\n",
      "Epoch 49 of 50 took 0.148s\n",
      "  training loss:\t\t0.695873\n",
      "Epoch 50 of 50 took 0.152s\n",
      "  training loss:\t\t0.690084\n",
      "0.81765951013\n",
      "Starting training... fold 12 of 10\n",
      "Epoch 1 of 50 took 0.145s\n",
      "  training loss:\t\t2.144921\n",
      "Epoch 2 of 50 took 0.147s\n",
      "  training loss:\t\t1.279336\n",
      "Epoch 3 of 50 took 0.150s\n",
      "  training loss:\t\t1.034059\n",
      "Epoch 4 of 50 took 0.146s\n",
      "  training loss:\t\t0.924684\n",
      "Epoch 5 of 50 took 0.145s\n",
      "  training loss:\t\t0.873875\n",
      "Epoch 6 of 50 took 0.145s\n",
      "  training loss:\t\t0.838014\n",
      "Epoch 7 of 50 took 0.145s\n",
      "  training loss:\t\t0.815497\n",
      "Epoch 8 of 50 took 0.145s\n",
      "  training loss:\t\t0.806507\n",
      "Epoch 9 of 50 took 0.145s\n",
      "  training loss:\t\t0.792101\n",
      "Epoch 10 of 50 took 0.145s\n",
      "  training loss:\t\t0.781280\n",
      "Epoch 11 of 50 took 0.145s\n",
      "  training loss:\t\t0.772724\n",
      "Epoch 12 of 50 took 0.146s\n",
      "  training loss:\t\t0.764868\n",
      "Epoch 13 of 50 took 0.146s\n",
      "  training loss:\t\t0.759651\n",
      "Epoch 14 of 50 took 0.146s\n",
      "  training loss:\t\t0.760065\n",
      "Epoch 15 of 50 took 0.147s\n",
      "  training loss:\t\t0.745783\n",
      "Epoch 16 of 50 took 0.147s\n",
      "  training loss:\t\t0.751799\n",
      "Epoch 17 of 50 took 0.146s\n",
      "  training loss:\t\t0.744483\n",
      "Epoch 18 of 50 took 0.146s\n",
      "  training loss:\t\t0.741934\n",
      "Epoch 19 of 50 took 0.145s\n",
      "  training loss:\t\t0.737693\n",
      "Epoch 20 of 50 took 0.145s\n",
      "  training loss:\t\t0.729160\n",
      "Epoch 21 of 50 took 0.145s\n",
      "  training loss:\t\t0.731500\n",
      "Epoch 22 of 50 took 0.145s\n",
      "  training loss:\t\t0.732230\n",
      "Epoch 23 of 50 took 0.145s\n",
      "  training loss:\t\t0.729570\n",
      "Epoch 24 of 50 took 0.145s\n",
      "  training loss:\t\t0.725376\n",
      "Epoch 25 of 50 took 0.145s\n",
      "  training loss:\t\t0.720447\n",
      "Epoch 26 of 50 took 0.145s\n",
      "  training loss:\t\t0.725149\n",
      "Epoch 27 of 50 took 0.145s\n",
      "  training loss:\t\t0.720216\n",
      "Epoch 28 of 50 took 0.145s\n",
      "  training loss:\t\t0.716766\n",
      "Epoch 29 of 50 took 0.145s\n",
      "  training loss:\t\t0.716518\n",
      "Epoch 30 of 50 took 0.145s\n",
      "  training loss:\t\t0.713394\n",
      "Epoch 31 of 50 took 0.144s\n",
      "  training loss:\t\t0.711031\n",
      "Epoch 32 of 50 took 0.145s\n",
      "  training loss:\t\t0.714636\n",
      "Epoch 33 of 50 took 0.145s\n",
      "  training loss:\t\t0.707643\n",
      "Epoch 34 of 50 took 0.145s\n",
      "  training loss:\t\t0.705102\n",
      "Epoch 35 of 50 took 0.145s\n",
      "  training loss:\t\t0.707323\n",
      "Epoch 36 of 50 took 0.146s\n",
      "  training loss:\t\t0.711314\n",
      "Epoch 37 of 50 took 0.145s\n",
      "  training loss:\t\t0.705021\n",
      "Epoch 38 of 50 took 0.144s\n",
      "  training loss:\t\t0.701711\n",
      "Epoch 39 of 50 took 0.145s\n",
      "  training loss:\t\t0.697734\n",
      "Epoch 40 of 50 took 0.145s\n",
      "  training loss:\t\t0.701708\n",
      "Epoch 41 of 50 took 0.145s\n",
      "  training loss:\t\t0.700309\n",
      "Epoch 42 of 50 took 0.145s\n",
      "  training loss:\t\t0.700847\n",
      "Epoch 43 of 50 took 0.144s\n",
      "  training loss:\t\t0.698405\n",
      "Epoch 44 of 50 took 0.144s\n",
      "  training loss:\t\t0.691912\n",
      "Epoch 45 of 50 took 0.145s\n",
      "  training loss:\t\t0.694441\n",
      "Epoch 46 of 50 took 0.145s\n",
      "  training loss:\t\t0.695627\n",
      "Epoch 47 of 50 took 0.145s\n",
      "  training loss:\t\t0.690056\n",
      "Epoch 48 of 50 took 0.144s\n",
      "  training loss:\t\t0.692097\n",
      "Epoch 49 of 50 took 0.145s\n",
      "  training loss:\t\t0.693600\n",
      "Epoch 50 of 50 took 0.145s\n",
      "  training loss:\t\t0.686573\n",
      "0.808414043584\n",
      "0.81289304162 0.00373924813839\n"
     ]
    }
   ],
   "source": [
    "f = 1\n",
    "accuracies = []\n",
    "for train_index, test_index in cv:\n",
    "    \n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    y_train = y_train.astype('int32')\n",
    "    y_test = y_test.astype('int32')\n",
    "    \n",
    "    net = {}\n",
    "    net['input'] = InputLayer((None, 1, 160),input_var=input_var)\n",
    "    net['fc1'] = DenseLayer(net['input'], num_units=1000,\n",
    "                            nonlinearity = lasagne.nonlinearities.rectify,\n",
    "                            W = lasagne.init.GlorotUniform(gain = 'relu'))\n",
    "    net['drop1'] = DropoutLayer(net['fc1'], p=0.5)\n",
    "    net['fc4'] = DenseLayer(net['drop1'], num_units=200,\n",
    "                            nonlinearity = lasagne.nonlinearities.rectify,\n",
    "                            W = lasagne.init.GlorotUniform(gain = 'relu'))\n",
    "    net['drop4'] = DropoutLayer(net['fc4'], p=0.5)\n",
    "    net['fc5'] = DenseLayer(net['drop4'], num_units=20, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    output_layer = net['fc5']\n",
    "    \n",
    "    prediction = lasagne.layers.get_output(output_layer)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    params = lasagne.layers.get_all_params(output_layer, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.05, momentum=0.9)\n",
    "\n",
    "    test_prediction = lasagne.layers.get_output(output_layer, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    test_fn = theano.function([input_var], test_prediction)\n",
    "    \n",
    "    num_epochs = 40\n",
    "    batchsize = 2500\n",
    "    best_epoch = 0\n",
    "    best_acc = 0\n",
    "    best_out = np.zeros((len(X_test), 20))\n",
    "\n",
    "    print(\"Starting training... fold {:} of {:}\".format(f, 10))\n",
    "    f += 1\n",
    "    for epoch in range(num_epochs):\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, batchsize, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "            \n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        #print(\"  training accuracy:\\t\\t{:.6f}\".format(train_acc / train_batches))\n",
    "        #print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        #print(\"  validation accuracy:\\t\\t{:.6f}\".format(val_acc / val_batches))\n",
    "    \n",
    "    preds = []\n",
    "    for t in X_test:\n",
    "        vals = test_fn(t.reshape((1, 1, 160)))\n",
    "        preds.append(vals)\n",
    "        \n",
    "    preds = np.array(preds).reshape((-1, 20)).argmax(axis = 1)\n",
    "        \n",
    "    print accuracy_score(y_test, preds)\n",
    "    accuracies.append(accuracy_score(y_test, preds))\n",
    "        \n",
    "print np.mean(accuracies), np.std(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multi:softmax\", \n",
    "          \"booster\": \"gbtree\",\n",
    "          \"nthread\": 4,\n",
    "          \"eta\": 0.5, \n",
    "          \"max_depth\": 3, \n",
    "          \"subsample\": 0.9, \n",
    "          \"colsample_bytree\": 0.5,\n",
    "          \"num_class\": 20,\n",
    "          \"silent\": 1\n",
    "         } \n",
    "num_trees = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805538832029\n",
      "0.802227573751\n",
      "0.806626506024\n",
      "0.803496081977\n",
      "0.804098854732\n",
      "0.797044632087\n",
      "0.80356065178\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-a1af06dffc0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#dtest = xgb.DMatrix(test[features])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mgbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/xgboost-0.4-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, early_stopping_rounds, evals_result, verbose_eval, learning_rates)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mbst_eval_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst_eval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst_eval_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/xgboost-0.4-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36meval_set\u001b[1;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[0;32m    741\u001b[0m             _check_call(_LIB.XGBoosterEvalOneIter(self.handle, iteration,\n\u001b[0;32m    742\u001b[0m                                                   \u001b[0mdmats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m                                                   ctypes.byref(msg)))\n\u001b[0m\u001b[0;32m    744\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = 1\n",
    "accuracies = []\n",
    "for train_index, test_index in cv:\n",
    "    \n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dvalid = xgb.DMatrix(X_test)\n",
    "    #dtest = xgb.DMatrix(test[features])\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, verbose_eval=False)\n",
    "    \n",
    "    preds = gbm.predict(dvalid)\n",
    "        \n",
    "    print accuracy_score(y_test, preds)\n",
    "    accuracies.append(accuracy_score(y_test, preds))\n",
    "        \n",
    "print np.mean(accuracies), np.std(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823600240819\n",
      "0.815171583384\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-a0ff8fd28ee7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1205\u001b[0m                       \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1207\u001b[1;33m             for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    660\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mlogistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, copy, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\u001b[0m\n\u001b[0;32m    691\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m                     iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)\n\u001b[0m\u001b[0;32m    694\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[1;31m# old scipy doesn't have maxiter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[1;32m--> 188\u001b[1;33m                            **opts)\n\u001b[0m\u001b[0;32m    189\u001b[0m     d = {'grad': res['jac'],\n\u001b[0;32m    190\u001b[0m          \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, **unknown_options)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# minimization routine wants f and g at the current x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                 \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[1;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mz0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Case where we fit the intercept.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = 1\n",
    "accuracies = []\n",
    "for train_index, test_index in cv:\n",
    "    \n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    clf = LogisticRegression(C=4,penalty='l2',random_state=0,solver='lbfgs',multi_class='ovr')\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    preds = clf.predict(X_test)\n",
    "        \n",
    "    print accuracy_score(y_test, preds)\n",
    "    accuracies.append(accuracy_score(y_test, preds))\n",
    "        \n",
    "print np.mean(accuracies), np.std(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81565257,  0.81454107,  0.8148204 ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LogisticRegression(C=4,penalty='l2',random_state=0,solver='lbfgs',multi_class='ovr'), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
